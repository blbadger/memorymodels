import matplotlib.pyplot as plt
import numpy as np
import torch
import json
import html
import webbrowser
from transformers import AutoTokenizer
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

loss = [1.6812, 1.7499, 2.4015, 2.5973, 2.7348, 2.7833, 2.5104, 2.4677, 2.4034,
        1.8831, 2.6912, 3.0662, 2.3064, 2.4403, 3.3005, 2.4798, 2.1564, 2.0680,
        2.1928, 2.5160, 2.3451, 2.1815, 2.1676, 2.6060, 1.7845, 2.7563, 2.3731,
        2.4199, 2.6321, 2.9644, 1.9308, 2.5800, 2.5837, 2.3612, 2.6995, 2.5704,
        2.2295, 2.4081, 2.4450, 2.6959, 2.9077, 2.4390, 3.2034, 2.4399, 2.6407,
        2.6784, 2.0462, 2.6051, 2.3370, 2.0200, 2.2988, 2.3667, 2.8919, 2.8845,
        3.2632, 2.3938, 2.5804, 2.5536, 2.6459, 2.3328, 2.4533, 2.8875, 1.9994,
        2.1073]

length = [1023,  639,  241, 1023,  212,  556, 1023,  933,  537, 1023,  615, 1023,
         980, 1023,  190,  644,  721,  723, 1023, 1023, 1023,  527, 1023,  646,
         515,  330,  479, 1023,  790,  273,  582,  169,  318, 1023,  356, 1023,
         738,  659,  486,  949,  581, 1023,  816,  329,  281,  402, 1023,  605,
        1023,  430, 1023,  327,  290, 1023,  347,  758,  416,  309,  758, 1023,
         875,  760, 1023,  509]

loss = [ 9.0282, 10.8955, 11.5910,  2.1424, 11.8171,  6.7268,  9.2251,  2.5711,
        11.0325, 10.1141,  9.8485, 12.0035, 11.2991,  2.5180, 11.9254, 11.6557,
        12.8561,  8.0350,  7.0327,  2.3580, 11.1995, 12.1181,  3.9103,  8.8894,
         2.6689, 11.7325, 12.4987, 10.7803, 11.0932,  2.4969,  9.1540,  2.2549,
        11.7617, 11.9875,  2.9657, 11.0062, 11.1007, 11.5567,  2.7783,  6.5652,
         2.6085,  8.3973, 11.4769, 10.5922,  2.1910, 11.1822, 10.9562, 10.9634,
         2.4748,  2.5582, 11.3812,  2.5846, 11.1013,  8.2469, 12.0999,  2.6691,
        11.0160, 11.8585, 12.9040,  2.3581,  1.9885,  7.4213,  2.4310,  2.0624],
length = [ 661,  416,  499, 1023,  222,  867,  794, 1023,  553,  347,  754,  291,
         423, 1023,  106,  319,  148,  941,  956, 1023,  225,  397, 1021,  578,
        1023,  389,  352,  603,  337, 1023,  680, 1023,  391,  548, 1021,  219,
         440,  249, 1023,  956, 1023,  794,  345,  375, 1023,  440,  538,  587,
        1023, 1023,  583, 1023,  288,  858,  224, 1023,  214,  237,  481, 1023,
        1023,  934, 1023, 1023]
all_losses = loss
all_attr = length 
plt.scatter(loss, length)
plt.show()
plt.close()

model = LinearRegression()
model.fit(np.array(all_losses).reshape(-1, 1), np.array(all_attr))

slope = model.coef_[0]
bias = model.intercept_
r2_score_method = model.score(np.array(all_losses).reshape(-1, 1), np.array(all_attr))
y_pred = model.predict(np.array(all_losses).reshape(-1, 1))
r2_score_func = r2_score(np.array(all_attr), y_pred)

print(f"Slope (coefficient): {slope:.4f}")
print(f"Bias (intercept): {bias:.4f}")
print(f"R-squared (from model.score()): {r2_score_method:.4f}")
print(f"R-squared (from sklearn.metrics.r2_score): {r2_score_func:.4f}")
